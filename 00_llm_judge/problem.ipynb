{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "\n",
        "from openai import OpenAI\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "0vYoDxs_Y9fv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLM:\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.llm = OpenAI(api_key=os.getenv(\"OPENAI_KEY\"))\n",
        "        self.model = model\n",
        "\n",
        "    def response(self, chat, temp=0, max_tokens=500):\n",
        "\n",
        "        response = self.llm.chat.completions.create(model=self.model,\n",
        "                                                        messages=chat,\n",
        "                                                        temperature=temp,\n",
        "                                                        max_tokens=max_tokens)\n",
        "        return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "F7tS_1z-ZARn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answer(answer):\n",
        "    ix = answer.rindex(\"####\")\n",
        "    return answer[ix + len(\"####\"):].strip()"
      ],
      "metadata": {
        "id": "sUeykCOhZCQb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 of the problem -- generate"
      ],
      "metadata": {
        "id": "6qJoNgWvZUiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(test_dataset, llm):\n",
        "    \"\"\"\n",
        "    :param test_dataset: You are given a test_dataset which is a list of problem description\n",
        "    :param llm: The LLM to be used\n",
        "    :return: a list of (reasoning, answer)\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "sqbocM1ZZFFo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(response_answer, gold_answers):\n",
        "\n",
        "    predictions = [answer for (reasoning, answer) in response_answer]\n",
        "    scores = [1.0 if answer.lower().strip() == gold_answer.lower().strip() else 0.0\n",
        "              for (answer, gold_answer) in zip(predictions, gold_answers)]\n",
        "    return scores"
      ],
      "metadata": {
        "id": "AqaiXO2eZG8K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 of the problem"
      ],
      "metadata": {
        "id": "-ZX9YMcSZXQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_judge_response(test_dataset_with_response):\n",
        "    \"\"\"\n",
        "    :param test_dataset_with_response: You are given the train_dataset which is a list of tuple (problem description, llm reasoning)\n",
        "    :return: a list of (judge_response, judge's answer which is True or Wrong)\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "muztn-W7ZJRa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3 of the problem -- evaluate the LLM judge"
      ],
      "metadata": {
        "id": "qIzm9YQ-ZbMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_llm_judge(predictions, gold_eval):\n",
        "    \"\"\"\n",
        "    :param predictions: A list of 0-1 score whether a given prediction is right (1) or wrong (0)\n",
        "    :param gold_eval: A list of ground truth scores whether a given prediction is right (1) or wrong (0)\n",
        "    :return: overall\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "yacC6oBSZLCK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4 of the problem -- self-reflection if the LLM judge produced a wrong answer"
      ],
      "metadata": {
        "id": "XQd6EmEHZdAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_reflection(history):\n",
        "    \"\"\"\n",
        "    :param history: A single datapoint's history of original question, model's first reasoning, and llm judge response\n",
        "    :return: output both the final reasoning and the extracted final answer\n",
        "    \"\"\"\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "D0xdglv9ZNZv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(histories):\n",
        "\n",
        "    fieldnames = [\"question\", \"model_reasoning\", \"judge_reasoning\", \"reflection_reasoning\"]\n",
        "    with open(\"./history.csv\", \"w\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for history in histories:\n",
        "            row = {k: v for (k, v) in zip(fieldnames, history)}\n",
        "            writer.writerow(row)"
      ],
      "metadata": {
        "id": "KVrNXaM5ZQQC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQsjWw9yY4og"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    # Load the GSM8k dataset. This is a set of school math puzzle problems where given a math problem in text\n",
        "    # you have to generate an answer.\n",
        "    # https://huggingface.co/datasets/openai/gsm8k\n",
        "    dataset = load_dataset(\"openai/gsm8k\", \"main\")\n",
        "    test_dataset = [(dp[\"question\"], extract_answer(dp[\"answer\"])) for dp in dataset[\"test\"]]\n",
        "    #test_dataset = test_dataset[:20] # test with the first 20 examples\n",
        "    gold_answers = [predicted_answer for (_, predicted_answer) in test_dataset]\n",
        "\n",
        "    # Step 1: Generate responses and check the answer\n",
        "    llm = LLM(model=\"TODO\")\n",
        "\n",
        "    reasoning_with_answer = generate_response(test_dataset, llm)\n",
        "\n",
        "    model_scores = eval_model(reasoning_with_answer, gold_answers)\n",
        "    model_acc = (sum(model_scores) * 100.0) / float(len(model_scores))\n",
        "    print(f\"Accuracy of the LLM agent is {model_acc:.2f}\")\n",
        "\n",
        "    # Step 2: Generate LLM judge response\n",
        "    test_dataset_with_response = [(dp[0], reasoning_with_answer_[0])\n",
        "                                  for reasoning_with_answer_, dp in zip(reasoning_with_answer, test_dataset)]\n",
        "    judge_response_with_answer = generate_judge_response(test_dataset_with_response)\n",
        "    judge_answers = [judge_answer for (judge_response, judge_answer) in judge_response_with_answer]\n",
        "\n",
        "    # Step 3: Evaluate LLM judge response\n",
        "    # - judge_predictions are 0/1 whether the LLM judge thinks the datapoint was answered correctly or not\n",
        "    # - model_scores contains 0/1 score on whether each datapoint was answered correctly or not\n",
        "    # use these two to evaluate the LLM judge\n",
        "    judge_scores = eval_llm_judge(judge_answers, model_scores)\n",
        "    judge_acc = (sum(judge_scores) * 100.0) / float(len(model_scores))\n",
        "    print(f\"Accuracy of the LLM Judge is {judge_acc:.2f}\")\n",
        "\n",
        "    # Step 4: for those datapoints where the LLM judge predicted the model's response to be wrong\n",
        "    # now prompt the model to recompute their response given the original prompt, model's initial response, and\n",
        "    # judge's predictions all put in a chat history.\n",
        "\n",
        "    final_scores = []\n",
        "    full_histories = [] # for debugging\n",
        "    for (dp, reasoning_with_answer_, judge_response_with_answer_) in \\\n",
        "            zip(test_dataset, reasoning_with_answer, judge_response_with_answer):\n",
        "\n",
        "        model_reasoning, model_answer = reasoning_with_answer_\n",
        "        judge_reasoning, judge_answer = judge_response_with_answer_\n",
        "\n",
        "        # History of this datapoint containing the first question, model's reasoning and then the judge's reasoning\n",
        "        history = [dp[\"question\"], model_reasoning, judge_reasoning]\n",
        "\n",
        "        if judge_answer == 1:  # Correct\n",
        "            final_answer = model_answer\n",
        "            history.append(\"N/A\")   # N/A indicating no self-reflection reasoning\n",
        "        else:\n",
        "            # Self-reflection step: LLM judge thinks we made a mistake answering this datapoint, so retry\n",
        "            reflection_reasoning, final_answer = self_reflection(history)\n",
        "            history.append(reflection_reasoning)\n",
        "\n",
        "        final_score = 1.0 if dp[1].lower().strip() == final_answer.lower().strip() else 0\n",
        "        final_scores.append(final_score)\n",
        "        full_histories.append(history)\n",
        "\n",
        "    final_acc = (sum(final_scores) * 100.0) / float(len(final_scores))\n",
        "    print(f\"Final Accuracyy of the Self-Reflection LLM Agent is {final_acc:.2f}\")\n",
        "\n",
        "    save(full_histories)\n",
        "\n",
        "    if final_acc > model_acc:\n",
        "        print(\"You were able to achieve higher final accuracy using self-reflection than the original model.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}